{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef4568f2",
   "metadata": {},
   "source": [
    "# Example 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd57cbb3",
   "metadata": {},
   "source": [
    "Agent 1\n",
    "Desc: Access the data and identify the groups\n",
    "tool(s):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc30fe5c",
   "metadata": {},
   "source": [
    "upload data to postgres table with this command: \n",
    "\n",
    "```\n",
    "psql \\                            \n",
    "  --host localhost \\\n",
    "  --port 5433 \\\n",
    "  --username devuser \\\n",
    "  --dbname devdb \\\n",
    "  --command \"\\\n",
    "\\\\copy public.nodes(id, type, tags, lat, lon) \\  \n",
    "  FROM '<full-path-to-csv>' \\    \n",
    "  WITH (FORMAT csv, HEADER true)\"    \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a671c076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # this reads .env and injects into os.environ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c573c54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xb/kjyjn5kx61950xxgm7klgb800000gn/T/ipykernel_36367/1200068300.py:23: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  chat = ChatOpenAI(model=MODEL_STR, temperature=0)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# classification_agent.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from langchain.tools import Tool\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# 1. Load environment (for OPENAI_API_KEY, optional DB_URL override)\n",
    "load_dotenv()\n",
    "\n",
    "# 2. Configuration\n",
    "DB_URL    = os.getenv(\n",
    "    \"DB_URL\",\n",
    "    \"postgresql+psycopg2://devuser:devpassword@localhost:5433/devdb\"\n",
    ")\n",
    "MODEL_STR = os.getenv(\"OPENAI_MODEL\", \"openai:gpt-4o\")\n",
    "\n",
    "\n",
    "# 3. Global storage variable for detected columns\n",
    "classification_columns: list[str] = []\n",
    "\n",
    "# 4. Detection logic: read table, return column names\n",
    "\n",
    "def detect_classification_columns(table_name: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Load `table_name` into a DataFrame and return a list of columns\n",
    "    that are not IDs (col=='id' or ending '_id') and not numeric.\n",
    "    Also updates global `classification_columns`.\n",
    "    \"\"\"\n",
    "    global classification_columns\n",
    "    engine = create_engine(DB_URL)\n",
    "    df = pd.read_sql_table(table_name, engine)\n",
    "    cols = []\n",
    "    for col in df.columns:\n",
    "        lc = col.lower()\n",
    "        if lc == \"id\" or lc.endswith(\"_id\"):\n",
    "            continue\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            continue\n",
    "        cols.append(col)\n",
    "    classification_columns = cols\n",
    "    return cols\n",
    "\n",
    "# 5. Wrap detection as a LangChain Tool\n",
    "detect_tool = Tool(\n",
    "    name=\"detect_classification_columns\",\n",
    "    func=detect_classification_columns,\n",
    "    description=(\n",
    "        \"Given a Postgres table name, return non-ID, non-numeric columns \"\n",
    "        \"for classification and store them in `classification_columns`.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 6. Initialize LLM for tool-binding\n",
    "tt_model = init_chat_model(MODEL_STR, temperature=0)\n",
    "\n",
    "# 7. Create the React agent\n",
    "agent = create_react_agent(\n",
    "    model=tt_model,\n",
    "    tools=[detect_tool],\n",
    "    prompt=(\n",
    "        \"You are an agent that receives a SQL table name, detects which columns \"\n",
    "        \"are useful for classification, and stores them in the global variable.\"\n",
    "    ),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "db6ac698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(model='gpt-4o', temperature=0)\n",
    "gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b952658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Detecting classification columns for table: nodes\n",
      "\n",
      "Agent response:\n",
      " {'messages': [HumanMessage(content='nodes', additional_kwargs={}, response_metadata={}, id='c35281fb-e172-4447-8b15-3d5874c96e14'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_CTxypKWkmztGEeX6uDFjgDdG', 'function': {'arguments': '{\"__arg1\":\"nodes\"}', 'name': 'detect_classification_columns'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 99, 'total_tokens': 118, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_90122d973c', 'id': 'chatcmpl-BVbZ64Gnz9BfLi7XeDE88Cppzl3KT', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--b5161436-2c66-45ab-9c22-788c9a68dcb2-0', tool_calls=[{'name': 'detect_classification_columns', 'args': {'__arg1': 'nodes'}, 'id': 'call_CTxypKWkmztGEeX6uDFjgDdG', 'type': 'tool_call'}], usage_metadata={'input_tokens': 99, 'output_tokens': 19, 'total_tokens': 118, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='[\"type\", \"tags\"]', name='detect_classification_columns', id='ccea8d2d-5343-4e3a-b48f-0f14d739e844', tool_call_id='call_CTxypKWkmztGEeX6uDFjgDdG'), AIMessage(content='The useful columns for classification in the \"nodes\" table are \"type\" and \"tags\".', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 133, 'total_tokens': 154, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_90122d973c', 'id': 'chatcmpl-BVbZDJknSpNM6hiT2v7APXsFwI0MF', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--06855fd2-1371-490a-8bee-c989599f2e7d-0', usage_metadata={'input_tokens': 133, 'output_tokens': 21, 'total_tokens': 154, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]} \n",
      "\n",
      "Detected classification columns: ['type', 'tags']\n"
     ]
    }
   ],
   "source": [
    "table_name = \"nodes\"\n",
    "print(f\"\\n▶ Detecting classification columns for table: {table_name}\\n\")\n",
    "\n",
    "# Agent invocation will call our detect_tool\n",
    "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": table_name}]})\n",
    "print(\"Agent response:\\n\", response, \"\\n\")\n",
    "\n",
    "# classification_columns global now holds the detected columns\n",
    "print(\"Detected classification columns:\", classification_columns)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbd1a058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['type', 'tags']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "173f2e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "def get_semantic_group_names(\n",
    "    df: pd.DataFrame,\n",
    "    columns: List[str]\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    For each row in `df[columns]`, ask the LLM to assign a group name,\n",
    "    but *enforce* via StructuredOutputParser that the model emits exactly\n",
    "    a JSON object with a 'group_names' key and a JSON list of strings.\n",
    "    \"\"\"\n",
    "    # 1) Serialize just the fields you care about\n",
    "    records = df[columns].to_dict(orient=\"records\")\n",
    "    records_json = json.dumps(records, indent=2)\n",
    "\n",
    "    # 2) Prepare a single-schema parser\n",
    "    response_schemas = [\n",
    "        ResponseSchema(\n",
    "            name=\"group_names\",\n",
    "            description=(\n",
    "                \"A JSON array of the distinct semantic group names that \"\n",
    "                \"cover all input records (no duplicates).\"\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "    format_instructions = parser.get_format_instructions()\n",
    "\n",
    "    # 3) Build the prompt with strict format instructions\n",
    "    prompt = f\"\"\"\n",
    "You are given a JSON array of records, each with only these fields: {columns}\n",
    "\n",
    "{records_json}\n",
    "\n",
    "Your tasks:\n",
    "1. Analyze all the records to find the underlying semantic \"groups\" they form.\n",
    "2. Produce only a single JSON object matching this schema:\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Make sure:\n",
    "- You list each group name exactly once.\n",
    "- You do NOT output per-record labels, only the distinct set of group names.\n",
    "- You include no extra text or commentary.\n",
    "\"\"\"\n",
    "    # This builds a single pipeline: LLM → parser\n",
    "    chain = gemini | parser\n",
    "\n",
    "    output = chain.invoke(prompt)\n",
    "    # output is now a dict: {\"group_names\": [...]}\n",
    "    return output[\"group_names\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b22bc095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['motorway_junction', 'crossing_with_traffic_signals', 'speed_camera', 'junction_name', 'PGS_sourced', 'traffic_signals', 'crossing_marked', 'barrier', 'other']\n"
     ]
    }
   ],
   "source": [
    "def get_records(sample_size=1000, table_name=\"nodes\"):\n",
    "    engine = create_engine(DB_URL)\n",
    "    q = f\"SELECT * FROM {table_name} LIMIT {sample_size}\"\n",
    "    df = pd.read_sql(q, engine)\n",
    "    return df\n",
    "\n",
    "sample_size = 1000\n",
    "df = get_records(sample_size=sample_size)\n",
    "groups = get_semantic_group_names(df, classification_columns)\n",
    "print(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f0b863",
   "metadata": {},
   "source": [
    "## Different version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fc713f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "def get_id_category_pairs(\n",
    "    df: pd.DataFrame,\n",
    "    id_col: str,\n",
    "    group_cols: List[str],\n",
    ") -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Returns a list of (id, category) pairs for each record in df,\n",
    "    where `category` is the semantic group assigned by the LLM.\n",
    "    \n",
    "    Args:\n",
    "      df: the full DataFrame\n",
    "      id_col: name of the column holding each record's unique identifier\n",
    "      group_cols: list of columns to use when grouping/classifying\n",
    "    \"\"\"\n",
    "    # 1) Build records JSON including the ID\n",
    "    records = df[[id_col] + group_cols].to_dict(orient=\"records\")\n",
    "    records_json = json.dumps(records, indent=2)\n",
    "    \n",
    "    # 2) Define the response schema: array of {id: ..., category: ...}\n",
    "    response_schemas = [\n",
    "        ResponseSchema(\n",
    "            name=\"categorized_records\",\n",
    "            description=(\n",
    "                \"A JSON array of objects, each containing the original \"\n",
    "                f\"'{id_col}' and a new 'category' string assigned by the model.\"\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "    format_instructions = parser.get_format_instructions()\n",
    "    \n",
    "    # 3) Build the prompt with strict instructions\n",
    "    prompt = f\"\"\"\n",
    "You are given a JSON array of records, each with:\n",
    "  • \\\"{id_col}\\\": unique identifier  \n",
    "  • additional fields: {group_cols}\n",
    "\n",
    "Here are the records:\n",
    "{records_json}\n",
    "\n",
    "Your task:\n",
    "1. Analyze each record and assign it a concise semantic \\\"category\\\".\n",
    "2. Return **only** a single JSON object matching this schema:\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Example of required output format:\n",
    "```json\n",
    "{{\n",
    "  \"categorized_records\": [\n",
    "    {{ \"{id_col}\": \"123\", \"category\": \"Speed Camera\" }},\n",
    "    {{ \"{id_col}\": \"124\", \"category\": \"Toll Booth\" }},\n",
    "    …\n",
    "  ]\n",
    "}}\n",
    "No explanations, no extra fields—only the JSON object above.\n",
    "\"\"\"\n",
    "    # 4) Call the LLM → parser pipeline\n",
    "\n",
    "    chain = gemini | parser\n",
    "    output = chain.invoke(prompt)\n",
    "    recs = output[\"categorized_records\"]\n",
    "    print(recs)\n",
    "    return recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2cc15ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs = get_id_category_pairs(df, id_col=\"id\", group_cols=classification_columns)\n",
    "# for record_id, category in pairs:\n",
    "#     print(f\"{record_id} → {category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "276fe22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def cluster_and_map(\n",
    "    df: pd.DataFrame,\n",
    "    columns: List[str],\n",
    "    categories: List[str],\n",
    "    n_clusters: int\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of assigned categories (len == len(df)), by:\n",
    "     • embedding & KMeans-clustering into n_clusters\n",
    "     • LLM-mapping each cluster to one of your predefined categories\n",
    "    \"\"\"\n",
    "    # 1. Prepare texts + embeddings\n",
    "    def serialize(v):\n",
    "        try: obj = json.loads(v) if isinstance(v, str) else v\n",
    "        except: return str(v)\n",
    "        if isinstance(obj, dict):\n",
    "            return \"{\" + \", \".join(f\"{k}={json.dumps(obj[k],sort_keys=True)}\"\n",
    "                                   for k in sorted(obj)) + \"}\"\n",
    "        return str(obj)\n",
    "\n",
    "    texts = df[columns].applymap(serialize).agg(\" | \".join, axis=1).tolist()\n",
    "    embedder  = OpenAIEmbeddings()\n",
    "    embeddings = embedder.embed_documents(texts)\n",
    "\n",
    "    # 2. Cluster\n",
    "    km = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    clusters = km.fit_predict(embeddings)\n",
    "\n",
    "    # 3. Map clusters → categories via LLM\n",
    "    llm = gemini\n",
    "    cluster_to_cat = {}\n",
    "    for cid in range(n_clusters):\n",
    "        samples = [texts[i] for i,c in enumerate(clusters) if c==cid][:5]\n",
    "        prompt = f\"\"\"\n",
    "You have these candidate categories: {categories}\n",
    "\n",
    "Here are sample records for group {cid}:\n",
    "{chr(10).join(samples)}\n",
    "\n",
    "Which one of the above categories best describes *all* of these?\n",
    "Answer with exactly the category name.\n",
    "\"\"\"\n",
    "        cat = llm.predict(prompt).strip()\n",
    "        # fallback if the LLM picks something unknown\n",
    "        if cat not in categories:\n",
    "            cat = \"unknown\"\n",
    "        cluster_to_cat[cid] = cat\n",
    "\n",
    "    # 4. Assign back to each record\n",
    "    return [cluster_to_cat[c] for c in clusters]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a6367dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xb/kjyjn5kx61950xxgm7klgb800000gn/T/ipykernel_36367/2431647934.py:29: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  texts = df[columns].applymap(serialize).agg(\" | \".join, axis=1).tolist()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assigned_category            \n",
      "other                            784\n",
      "PGS_sourced                      166\n",
      "traffic_signals                   18\n",
      "crossing_with_traffic_signals     15\n",
      "motorway_junction                 11\n",
      "barrier                            4\n",
      "junction_name                      2\n",
      "Name: count, dtype: int64\n",
      "       id  type tags        lat       lon assigned_category\n",
      "0  123379  node   {}  51.200308  4.377739             other\n",
      "1  123380  node   {}  51.199611  4.380116             other\n",
      "2  123381  node   {}  51.199706  4.381602             other\n",
      "3  123382  node   {}  51.199627  4.383552             other\n",
      "4  123383  node   {}  51.199074  4.384953             other\n"
     ]
    }
   ],
   "source": [
    "assigned_labels = cluster_and_map(\n",
    "    df,\n",
    "    columns=classification_columns,\n",
    "    categories=groups,\n",
    "    n_clusters=len(groups)\n",
    ")\n",
    "\n",
    "# Attach back to your DataFrame\n",
    "df[\"assigned_category\"] = assigned_labels\n",
    "\n",
    "# Inspect\n",
    "print(df[[\"assigned_category\"]].value_counts())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bf1bc9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>tags</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>assigned_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>636413</td>\n",
       "      <td>node</td>\n",
       "      <td>{'name': 'Antwerpen-Oost', 'highway': 'motorwa...</td>\n",
       "      <td>51.216691</td>\n",
       "      <td>4.453695</td>\n",
       "      <td>motorway_junction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>636419</td>\n",
       "      <td>node</td>\n",
       "      <td>{'name': 'Antwerpen-Oost', 'highway': 'motorwa...</td>\n",
       "      <td>51.213587</td>\n",
       "      <td>4.448103</td>\n",
       "      <td>motorway_junction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>21280953</td>\n",
       "      <td>node</td>\n",
       "      <td>{'ref': '6', 'name': 'Wilrijk', 'highway': 'mo...</td>\n",
       "      <td>51.167431</td>\n",
       "      <td>4.413595</td>\n",
       "      <td>motorway_junction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>21281161</td>\n",
       "      <td>node</td>\n",
       "      <td>{'name': 'Antwerpen-Zuid', 'highway': 'motorwa...</td>\n",
       "      <td>51.188260</td>\n",
       "      <td>4.415757</td>\n",
       "      <td>motorway_junction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>25924005</td>\n",
       "      <td>node</td>\n",
       "      <td>{'ref': '5a', 'name': 'Antwerpen-Centrum', 'hi...</td>\n",
       "      <td>51.201224</td>\n",
       "      <td>4.375957</td>\n",
       "      <td>motorway_junction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>25924349</td>\n",
       "      <td>node</td>\n",
       "      <td>{'ref': '5a', 'name': 'Antwerpen-Centrum', 'hi...</td>\n",
       "      <td>51.197136</td>\n",
       "      <td>4.389047</td>\n",
       "      <td>motorway_junction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>25924420</td>\n",
       "      <td>node</td>\n",
       "      <td>{'name': 'Antwerpen-Zuid', 'highway': 'motorwa...</td>\n",
       "      <td>51.191967</td>\n",
       "      <td>4.403142</td>\n",
       "      <td>motorway_junction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>25924523</td>\n",
       "      <td>node</td>\n",
       "      <td>{'ref': '5', 'name': 'Le Grellelaan', 'highway...</td>\n",
       "      <td>51.190114</td>\n",
       "      <td>4.407187</td>\n",
       "      <td>motorway_junction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>25924658</td>\n",
       "      <td>node</td>\n",
       "      <td>{'ref': '4', 'name': 'Berchem', 'highway': 'mo...</td>\n",
       "      <td>51.193246</td>\n",
       "      <td>4.429339</td>\n",
       "      <td>motorway_junction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>25924707</td>\n",
       "      <td>node</td>\n",
       "      <td>{'ref': '3', 'name': 'Borgerhout', 'highway': ...</td>\n",
       "      <td>51.204976</td>\n",
       "      <td>4.440158</td>\n",
       "      <td>motorway_junction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>25947647</td>\n",
       "      <td>node</td>\n",
       "      <td>{'ref': '5a', 'name': 'Antwerpen-Centrum', 'no...</td>\n",
       "      <td>51.186392</td>\n",
       "      <td>4.415668</td>\n",
       "      <td>motorway_junction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  type                                               tags  \\\n",
       "80     636413  node  {'name': 'Antwerpen-Oost', 'highway': 'motorwa...   \n",
       "84     636419  node  {'name': 'Antwerpen-Oost', 'highway': 'motorwa...   \n",
       "133  21280953  node  {'ref': '6', 'name': 'Wilrijk', 'highway': 'mo...   \n",
       "137  21281161  node  {'name': 'Antwerpen-Zuid', 'highway': 'motorwa...   \n",
       "701  25924005  node  {'ref': '5a', 'name': 'Antwerpen-Centrum', 'hi...   \n",
       "732  25924349  node  {'ref': '5a', 'name': 'Antwerpen-Centrum', 'hi...   \n",
       "733  25924420  node  {'name': 'Antwerpen-Zuid', 'highway': 'motorwa...   \n",
       "750  25924523  node  {'ref': '5', 'name': 'Le Grellelaan', 'highway...   \n",
       "765  25924658  node  {'ref': '4', 'name': 'Berchem', 'highway': 'mo...   \n",
       "775  25924707  node  {'ref': '3', 'name': 'Borgerhout', 'highway': ...   \n",
       "952  25947647  node  {'ref': '5a', 'name': 'Antwerpen-Centrum', 'no...   \n",
       "\n",
       "           lat       lon  assigned_category  \n",
       "80   51.216691  4.453695  motorway_junction  \n",
       "84   51.213587  4.448103  motorway_junction  \n",
       "133  51.167431  4.413595  motorway_junction  \n",
       "137  51.188260  4.415757  motorway_junction  \n",
       "701  51.201224  4.375957  motorway_junction  \n",
       "732  51.197136  4.389047  motorway_junction  \n",
       "733  51.191967  4.403142  motorway_junction  \n",
       "750  51.190114  4.407187  motorway_junction  \n",
       "765  51.193246  4.429339  motorway_junction  \n",
       "775  51.204976  4.440158  motorway_junction  \n",
       "952  51.186392  4.415668  motorway_junction  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.loc[df[\"assigned_category\"] == 'motorway_junction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8ef2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "# --------------------------- helpers ---------------------------------\n",
    "def _looks_like_json(val) -> bool:\n",
    "    \"\"\"True if val is a dict or a JSON‑parsable string.\"\"\"\n",
    "    if isinstance(val, dict):\n",
    "        return True\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            obj = json.loads(val)\n",
    "            return isinstance(obj, dict)\n",
    "        except Exception:\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "def _auto_json_columns(df: pd.DataFrame, sample: int = 50) -> List[str]:\n",
    "    \"\"\"Return column names whose sample values are mostly dicts / JSON strings.\"\"\"\n",
    "    json_cols = []\n",
    "    for col in df.columns:\n",
    "        sample_vals = df[col].dropna().head(sample)\n",
    "        if sample_vals.empty:\n",
    "            continue\n",
    "        pct_json = sample_vals.map(_looks_like_json).mean()\n",
    "        if pct_json > 0.5:          # >50 % of sampled rows look like JSON\n",
    "            json_cols.append(col)\n",
    "    return json_cols\n",
    "\n",
    "def _flatten_json_series(s: pd.Series, prefix: str, sep=\"_\") -> pd.DataFrame:\n",
    "    \"\"\"Recursively flatten a JSON/dict series into scalar columns.\"\"\"\n",
    "    def flat(v, px=\"\"):\n",
    "        if isinstance(v, str):\n",
    "            try: v = json.loads(v)\n",
    "            except: return {px[:-1]: v}\n",
    "        if isinstance(v, dict):\n",
    "            out = {}\n",
    "            for k, val in v.items():\n",
    "                out.update(flat(val, f\"{px}{k}{sep}\"))\n",
    "            return out\n",
    "        return {px[:-1]: v}\n",
    "    return pd.json_normalize(s.map(lambda x: flat(x, f\"{prefix}{sep}\")))\n",
    "\n",
    "# ------------------- summarizer with auto‑detection -------------------\n",
    "def summarize_category_with_llm(\n",
    "    df: pd.DataFrame,\n",
    "    category: str,\n",
    "    llm,\n",
    "    id_col: str                = \"id\",\n",
    "    auto_sample: int           = 50         # rows per column to test for JSON\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    1) Filters df to `category`\n",
    "    2) Auto‑detects JSON‑like columns and flattens them\n",
    "    3) Computes per‑column stats\n",
    "    4) Returns stats + LLM narrative\n",
    "    \"\"\"\n",
    "    sub = df[df[\"assigned_category\"] == category].copy()\n",
    "    if sub.empty:\n",
    "        return {\"summary\": {}, \"narrative\": f\"No records for '{category}'.\"}\n",
    "\n",
    "    # 1⃣  auto‑detect and flatten all JSON‑ish columns\n",
    "    json_cols = _auto_json_columns(sub, sample=auto_sample)\n",
    "    flat_parts = [sub]\n",
    "    for jc in json_cols:\n",
    "        flat_parts.append(_flatten_json_series(sub[jc], jc))\n",
    "    wide = pd.concat(flat_parts, axis=1).drop(columns=json_cols)\n",
    "\n",
    "    # 2⃣  quick type-aware stats\n",
    "    summary = {\"category\": category, \"n_rows\": len(wide), \"columns\": {}}\n",
    "    for col in wide.columns:\n",
    "        if col in (\"assigned_category\", \"geometry\"):\n",
    "            continue\n",
    "        ser = wide[col].dropna()\n",
    "        if ser.empty:\n",
    "            summary[\"columns\"][col] = {\"all_null\": True}\n",
    "            continue\n",
    "        if ser.dtype.kind in \"if\":\n",
    "            summary[\"columns\"][col] = {\n",
    "                \"type\": \"numeric\",\n",
    "                \"min\": ser.min(),\n",
    "                \"max\": ser.max(),\n",
    "                \"mean\": ser.mean(),\n",
    "                \"p50\": ser.quantile(.5),\n",
    "                \"p95\": ser.quantile(.95),\n",
    "            }\n",
    "        elif ser.dtype == bool or ser.isin([0, 1]).all():\n",
    "            summary[\"columns\"][col] = {\n",
    "                \"type\": \"boolean\",\n",
    "                \"pct_true\": float(ser.mean()),\n",
    "            }\n",
    "        else:\n",
    "            top = ser.value_counts().head(5)\n",
    "            summary[\"columns\"][col] = {\n",
    "                \"type\": \"categorical\",\n",
    "                \"distinct\": int(ser.nunique()),\n",
    "                \"top_values\": top.to_dict(),\n",
    "            }\n",
    "\n",
    "    # 3⃣  Ask LLM for narrative\n",
    "    prompt = f\"\"\"\n",
    "You are a data analyst. Summarize these statistics for the category \"{category}\"\n",
    "in 3‑4 sentences, highlighting notable patterns.\n",
    "\n",
    "Stats JSON:\n",
    "{json.dumps(summary, indent=2)}\n",
    "\"\"\"\n",
    "    narrative = llm.predict(prompt).strip()\n",
    "\n",
    "    return {\"summary\": summary, \"narrative\": narrative}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5e9154",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = summarize_category_with_llm(df, \"motorway_junction\", gemini)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
